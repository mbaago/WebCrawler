\section{Crawling}
Our crawler is implemented in several classes, and is almost able to be properly multithreaded (if multiple instances were started, it would not be polite).
\begin{itemize}
	\item \texttt{Crawler}
	\item \texttt{Mercator}
	\item \texttt{RobotStuff}
\end{itemize}

\subsection{Crawler}
This class gets an URL to crawl from the mercator class and downloads it. The URL and downloaded content are enqueued for the indexer. A not-very-scientific test showed that downloading and enqueuing 500 .dk, starting with the seed (\url{newz.dk}, \url{aau.dk}, \url{politikken.dk}) sites took 4 minutes and 33 seconds on a 20/3 Mbit Stofa connection.

\subsection{Mercator}
This class is the implementation of the mercator scheme. It keeps a list of all URLs that have, at some point, been in a frontqueue, and only adds new URLs to a frontqueue if they are not in that list. It is implemented to use a custom number of front and back queues: we use 10 and 3, and use randomness to determine where new URLs go. Another possibility would be to give certain sites (e.g. forums) higher priority, to get the new content more often.

When the crawler asks for a page, it returns a site belonging to domain from a the oldest backqueue, sleeping if necessary, thereby implementing politeness. The mercator only returns URLs allowed from a domains robots.txt.

Using the mercator scheme helps enable politeness (contrary to a BFS), and prevent spider traps (contrary to DFS).

\subsection{RobotStuff}
This class handles robots.txt. When it is asked if it is allowed to visit and URL, it first determines if the robots.txt for that domain is not downloaded or too old. If so, the file is downloaded. It then determines if the URL may be visited. We cache the file for 5 minutes, but it is \emph{probably} not updated that often, so a longer time might be viable.