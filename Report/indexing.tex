\section{Indexing}
Because we are lazy, we use HtmlAgilityPack (\url{http://www.nuget.org/packages/HtmlAgilityPack}), and extract only headers and paragraphs from the downloaded HTML. We then determine if there is a near-duplicate (90\%) already in the database, using Jaccard similarity method with a shingle size of four words, and go on to next site if there is. Otherwise, we apply the following steps:

\begin{itemize}
    \item Process the text and try to make it danish.
	\item Tokenization.
    \item We do no normalization.
	\item Stop word removing.
	\item Case folding.
    \item Add the site to the index.
\end{itemize}

\subsection{Making the content danish}
As some characters are not represented properly when downloaded, we handle some instances of e.g. æ, ø, and å.

\subsection{Tokenization}
In this step we remove the unwanted characters shown in \Cref{sec:rem-chars}, and split the remainder on spaces, to get a collection of tokens.

\subsection{Stop words}
Using the list of stop words from \url{http://snowball.tartarus.org/algorithms/danish/stop.txt}, we remove them.

\subsection{Case folding}
We make all the tokens lowercase.

\subsection{Stemming}
We do not stem the tokens, but a possible algorithm to use would be \url{http://snowball.tartarus.org/algorithms/danish/stemmer.html}

\subsection{Adding the site to the index}
This is straightforward, by going through the sorted collection of tokens from the URL, and adding the URL and number of times the token appears on the site, to the tokens posting list.