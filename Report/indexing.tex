\section{Indexing}
Since we believe the focus of this project is not on parsing HTML, we have chosen to use HtmlAgilityPack (\url{http://www.nuget.org/packages/HtmlAgilityPack}), to save time. We then extract only headers and paragraphs from the downloaded HTML, again this is done to save time, but it would be a place where one could optimize the crawler in the future.

After the HTML have been parsed it is checked  to determine if there is a near-duplicate (90\%) already in the database, this is done using Jaccard similarity method with a shingle size of four words, if there already is a near-duplicate in the database, the process stops and it goes on to the next page. Otherwise, we apply the following steps:

\begin{itemize}
    \item Process the text and try to make it danish.
	\item Tokenization.
    \item We do no normalization.
	\item Stop word removing.
	\item Case folding.
    \item Add the site to the index.
\end{itemize}

\subsection{Making the content danish}
As some characters are not represented properly when downloaded, we handle some instances of e.g. æ, ø, and å.

\subsection{Tokenization}
In this step we remove the unwanted characters shown in \Cref{sec:rem-chars}, and split the remainder on spaces, to get a collection of tokens.

\subsection{Stop words}
Using the list of stop words from \url{http://snowball.tartarus.org/algorithms/danish/stop.txt}, we remove them.

\subsection{Case folding}
We make all the tokens lowercase.

\subsection{Stemming}
We do not stem the tokens, but a possible algorithm to use would be \url{http://snowball.tartarus.org/algorithms/danish/stemmer.html}

\subsection{Adding the site to the index}
This is straightforward, by going through the sorted collection of tokens from the URL, and adding the URL and number of times the token appears on the site, to the tokens posting list.